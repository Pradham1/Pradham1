{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPXmXVciVB9ii9/PY+8Wnkh",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Pradham1/Pradham1/blob/main/ProjectWebScraper.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from .ad import Ad\n",
        "from .ad import AdParser\n",
        "from .ad import fetch_ad\n",
        "\n",
        "from .search import Search\n",
        "from .search import SearchParser\n",
        "from .search import fetch_search\n",
        "\n",
        "from .utils import get_us_cities\n",
        "\n",
        "__version__ = \"1.1.2\""
      ],
      "metadata": {
        "id": "iDSIS17pm8XT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from bs4 import BeautifulSoup\n",
        "import requests\n",
        "import re\n",
        "\n",
        "from typing import Optional\n",
        "from typing import Union\n",
        "from typing import List\n",
        "from typing import Dict\n",
        "\n",
        "from .utils import format_price\n",
        "\n",
        "\n",
        "class Ad:\n",
        "    def __init__(\n",
        "        self,\n",
        "        url: str,\n",
        "        price: Optional[float] = None,\n",
        "        title: Optional[str] = None,\n",
        "        d_pid: Optional[int] = None,\n",
        "        description: Optional[str] = None,\n",
        "        attributes: Optional[Dict] = None,\n",
        "        image_urls: Optional[List[str]] = None\n",
        "    ) -> None:\n",
        "        \"\"\"An abstraction for a Craigslist 'Ad'. At the bare minimum you need a\n",
        "        `url` to define an ad. Although, at search-time, information such as the\n",
        "        price, title, and d_pid can additionallly be computed. If not provided,\n",
        "        these are computed lazily if the user fetches the ad information with\n",
        "        `ad.fetch()`.\n",
        "\n",
        "        \"\"\"\n",
        "        self.url = url\n",
        "        self.price = price\n",
        "        self.title = title\n",
        "        self.d_pid = d_pid\n",
        "        self.description = description\n",
        "        self.attributes = attributes\n",
        "        self.image_urls = image_urls\n",
        "\n",
        "    def __repr__(self) -> str:\n",
        "        if (self.title is None) or (self.price is None):\n",
        "            return f\"< {self.url} >\"\n",
        "\n",
        "        return f\"< {self.title} (${self.price}): {self.url} >\"\n",
        "\n",
        "    def fetch(self, **kwargs) -> int:\n",
        "        \"\"\"Fetch additional data from the url of the ad.\"\"\"\n",
        "        self.request = requests.get(self.url, **kwargs)\n",
        "        if self.request.status_code == 200:\n",
        "            parser = AdParser(self.request.content)\n",
        "            self.price = parser.price\n",
        "            self.title = parser.title\n",
        "            self.d_pid = parser.d_pid\n",
        "            self.description = parser.description\n",
        "            self.attributes = parser.attributes\n",
        "            self.image_urls = parser.image_urls\n",
        "            self.metadata = parser.metadata\n",
        "\n",
        "        return self.request.status_code\n",
        "\n",
        "    def to_dict(self) -> Dict:\n",
        "        return {\n",
        "            \"url\": self.url,\n",
        "            \"price\": self.price,\n",
        "            \"title\": self.title,\n",
        "            \"d_pid\": self.d_pid,\n",
        "            \"description\": self.description,\n",
        "            \"image_urls\": self.image_urls,\n",
        "            \"attributes\": self.attributes,\n",
        "        }\n",
        "\n",
        "\n",
        "def fetch_ad(url: str, **kwargs) -> Ad:\n",
        "    \"\"\"Functional way to fetch the ad information given a url.\"\"\"\n",
        "    ad = Ad(url = url)\n",
        "    ad.fetch(**kwargs)\n",
        "    return ad\n",
        "\n",
        "\n",
        "class AdParser:\n",
        "    def __init__(self, content: Union[str, bytes], **kwargs) -> None:\n",
        "        self.soup = BeautifulSoup(content, \"html.parser\", **kwargs)\n",
        "\n",
        "        # Remove QR text. This is important when parsing the description.\n",
        "        for qr in self.soup.find_all(\"p\", class_ = \"print-qrcode-label\"):\n",
        "            qr.decompose()\n",
        "\n",
        "    @property\n",
        "    def url(self) -> str:\n",
        "        return self.soup.find(\"meta\", property = \"og:url\")[\"content\"]\n",
        "\n",
        "    @property\n",
        "    def price(self) -> Optional[float]:\n",
        "        element = self.soup.find(\"span\", class_ = \"price\")\n",
        "        if element is not None:\n",
        "            return format_price(element.text)\n",
        "        return element\n",
        "\n",
        "    @property\n",
        "    def title(self) -> str:\n",
        "        return self.soup.find(\"span\", id = \"titletextonly\").text\n",
        "\n",
        "    @property\n",
        "    def d_pid(self) -> int:\n",
        "        return int(re.search(r\"/(\\d+)\\.html\", self.url).group(1))\n",
        "\n",
        "    @property\n",
        "    def description(self) -> str:\n",
        "        return self.soup.find(\"section\", id = \"postingbody\").text\n",
        "\n",
        "    @property\n",
        "    def attributes(self) -> Dict:\n",
        "        attrs = {}\n",
        "        for attr_group in self.soup.find_all(\"p\", class_ = \"attrgroup\"):\n",
        "            for attr in attr_group.find_all(\"span\"):\n",
        "                kv = attr.text.split(\": \")\n",
        "\n",
        "                # Add the attribute if and only if it's a key value attribute.\n",
        "                if len(kv) == 2: attrs[kv[0]] = kv[1]\n",
        "\n",
        "        return attrs\n",
        "\n",
        "    @property\n",
        "    def image_urls(self) -> List[str]:\n",
        "        return [a.get(\"href\") for a in self.soup.find_all(\"a\", class_ = \"thumb\")]\n",
        "\n",
        "    @property\n",
        "    def metadata(self) -> List[BeautifulSoup]:\n",
        "        return self.soup.find_all(\"meta\")\n",
        "\n"
      ],
      "metadata": {
        "id": "4qvtoeK3m9Dx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from bs4 import BeautifulSoup\n",
        "import requests\n",
        "import re\n",
        "\n",
        "from typing import Union\n",
        "from typing import List\n",
        "from typing import Dict\n",
        "\n",
        "from .ad import Ad\n",
        "from .utils import format_price\n",
        "from .utils import build_url\n",
        "\n",
        "\n",
        "class Search:\n",
        "    def __init__(self, query: str, city: str, category: str = \"sss\") -> None:\n",
        "        \"\"\"An abstraction for a Craigslist 'Search'. Similar to the 'Ad' this is\n",
        "        also lazy and follows the same layout with the `fetch()` and `to_dict()`\n",
        "        methods.\n",
        "\n",
        "        \"\"\"\n",
        "        self.query = query\n",
        "        self.city = city\n",
        "        self.category = category\n",
        "\n",
        "        self.url = build_url(self.query, self.city, self.category)\n",
        "        self.ads: List[Ad] = []\n",
        "\n",
        "    def fetch(self, **kwargs) -> int:\n",
        "        self.request = requests.get(self.url, **kwargs)\n",
        "        if self.request.status_code == 200:\n",
        "            parser = SearchParser(self.request.content)\n",
        "            self.ads = parser.ads\n",
        "\n",
        "        return self.request.status_code\n",
        "\n",
        "    def to_dict(self) -> Dict:\n",
        "        return {\n",
        "            \"query\": self.query,\n",
        "            \"city\": self.city,\n",
        "            \"category\": self.category,\n",
        "            \"url\": self.url,\n",
        "            \"ads\": [ad.to_dict() for ad in self.ads]\n",
        "        }\n",
        "\n",
        "\n",
        "def fetch_search(query: str, city: str, category: str = \"sss\", **kwargs) -> Search:\n",
        "    \"\"\"Functional implementation of a Craigslist search.\"\"\"\n",
        "    search = Search(query = query, city = city, category = category)\n",
        "    search.fetch(**kwargs)\n",
        "    return search\n",
        "\n",
        "\n",
        "class SearchParser:\n",
        "    def __init__(self, content: Union[str, bytes], **kwargs) -> None:\n",
        "        self.soup = BeautifulSoup(content, \"html.parser\", **kwargs)\n",
        "\n",
        "    @property\n",
        "    def ads(self) -> List[Ad]:\n",
        "        ads = []\n",
        "        for ad_html in self.soup.find_all(\"li\", class_ = \"cl-static-search-result\"):\n",
        "            url = ad_html.find(\"a\")[\"href\"]\n",
        "            title = ad_html.find(class_ = \"title\").text\n",
        "            price = format_price(ad_html.find(class_ = \"price\").text)\n",
        "            d_pid = int(re.search(r\"/(\\d+)\\.html\", url).group(1))\n",
        "\n",
        "            ads.append(\n",
        "                Ad(\n",
        "                    url = url,\n",
        "                    title = title,\n",
        "                    price = price,\n",
        "                    d_pid = d_pid\n",
        "                )\n",
        "            )\n",
        "\n",
        "        return ads\n",
        "\n"
      ],
      "metadata": {
        "id": "6HmLjuGqm_BV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from urllib.parse import quote\n",
        "import json\n",
        "import csv\n",
        "import os\n",
        "\n",
        "from typing import List\n",
        "from typing import Dict\n",
        "\n",
        "# Get the directory of the current file agnositc of library location.\n",
        "cs_dir = os.path.dirname(os.path.abspath(__file__))\n",
        "\n",
        "\n",
        "def get_us_cities() -> List[str]:\n",
        "    path = os.path.join(cs_dir, \"data/us_cities.csv\")\n",
        "    with open(path, \"r\") as file:\n",
        "        reader = csv.reader(file)\n",
        "        cities = [city[0] for city in reader]\n",
        "    return cities\n",
        "\n",
        "\n",
        "def format_price(price: str) -> float:\n",
        "    return float(price.replace(\"$\", \"\").replace(\",\", \"\"))\n",
        "\n",
        "\n",
        "def build_url(query: str, city: str, category: str = \"sss\") -> str:\n",
        "    return f\"https://{city}.craigslist.org/search/{category}?query={quote(query)}\"\n",
        "\n",
        "\n",
        "def get_areas() -> List[Dict]:\n",
        "    with open(os.path.join(cs_dir, \"data/areas.json\"), \"r\") as file:\n",
        "        areas = json.load(file)\n",
        "    return areas\n",
        "\n",
        "\n",
        "def get_categories() -> List[Dict]:\n",
        "    with open(os.path.join(cs_dir, \"data/categories.json\"), \"r\") as file:\n",
        "        categories = json.load(file)\n",
        "    return categories\n",
        "\n"
      ],
      "metadata": {
        "id": "KXulGm2cnRzD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install craigslistscraper"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "K-Z147xnnU3i",
        "outputId": "18bcf794-73a9-4549-c625-7fb88db2f158"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: craigslistscraper in /usr/local/lib/python3.11/dist-packages (1.1.2)\n",
            "Requirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.11/dist-packages (from craigslistscraper) (4.12.3)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from craigslistscraper) (2.32.3)\n",
            "Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.11/dist-packages (from beautifulsoup4->craigslistscraper) (2.6)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->craigslistscraper) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->craigslistscraper) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->craigslistscraper) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->craigslistscraper) (2024.12.14)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import craigslistscraper as cs\n",
        "import json\n",
        "\n",
        "# Define the search. Everything is done lazily, and so the html is not\n",
        "# fetched at this step.\n",
        "search = cs.Search(\n",
        "    query = \"bmw e46\",\n",
        "    city = \"minneapolis\",\n",
        "    category = \"cto\"\n",
        ")\n",
        "\n",
        "# Fetch the html from the server. Don't forget to check the status.\n",
        "status = search.fetch()\n",
        "if status != 200:\n",
        "    raise Exception(f\"Unable to fetch search with status <{status}>.\")\n",
        "\n",
        "print(f\"{len(search.ads)} ads found!\")\n",
        "for ad in search.ads:\n",
        "    # Fetch additional information about each ad. Check the status again.\n",
        "    status = ad.fetch()\n",
        "    if status != 200:\n",
        "        print(f\"Unable to fetch ad '{ad.title}' with status <{status}>.\")\n",
        "        continue\n",
        "\n",
        "    # There is a to_dict() method for convenience.\n",
        "    data = ad.to_dict()\n",
        "\n",
        "    # json.dumps is merely for pretty printing.\n",
        "    print(json.dumps(data, indent = 4))\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "luGi5iYEnr-T",
        "outputId": "889ac2f0-d461-4aa8-9e3e-53b5f7daa534"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0 ads found!\n"
          ]
        }
      ]
    }
  ]
}